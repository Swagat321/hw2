---
title: "HW 2 Student"
author: "Swagat Adhikary"
date: "9/27/2024"
output: 
  html_document:
    number_sections: true
---

This homework is meant to illustrate the methods of classification algorithms as well as their potential pitfalls.  In class, we demonstrated K-Nearest-Neighbors using the `iris` dataset.  Today I will give you a different subset of this same data, and you will train a KNN classifier.  

```{r, echo = FALSE}
set.seed(123)
library(class)

df <- data(iris) 

normal <-function(x) {
  (x -min(x))/(max(x)-min(x))   
}

iris_norm <- as.data.frame(lapply(iris[,c(1,2,3,4)], normal))

subset <- c(1:45, 58, 60:70, 82, 94, 110:150)
iris_train <- iris_norm[subset,] 
iris_test <- iris_norm[-subset,] 

iris_target_category <- iris[subset,5]
iris_test_category <- iris[-subset,5]


```

#
Above, I have given you a training-testing partition.  Train the KNN with $K = 5$ on the training data and use this to classify the 50 test observations.  Once you have classified the test observations, create a contingency table -- like we did in class -- to evaluate which observations your algorithm is misclassifying.   

```{r}
set.seed(123)
#STUDENT INPUT

pr <- knn(iris_train,iris_test,cl=iris_target_category,k=5)

tab <- table(pr,iris_test_category)
tab
accuracy <- function(x){
  sum(diag(x)/(sum(rowSums(x)))) * 100
}
accuracy(tab)


```

#

Discuss your results.  If you have done this correctly, you should have a classification error rate that is roughly 20% higher than what we observed in class.  Why is this the case? In particular run a summary of the `iris_test_category` as well as `iris_target_category` and discuss how this plays a role in your answer.  

*The reason that my classification error rate is 78% while the error rate observed in class was almost 97% is because in class we randomly sampled our data into a train/test partition whereas in this homework we were given a partition which was not randomly sampled. In particular, my test category contains a very large amount of Versicolor and target category contains very few Versicolor compared to the randomized sample from class which was more evenly distributed. This selective and non-representative subset of training data grossly misidentified the species and led to a roughly 20% higher error rate in my classification than in class.* 


#

Choice of $K$ can also influence this classifier.  Why would choosing $K = 6$ not be advisable for this data? 

*Choosing K=6 would not be advisable for this data because we should preferably choose a value for k that is indivisible by the number of class labels. A value of 6 is divisible by 3 (the number of species in this data), thus 6 is not a good option for k as a potential classification could result in a tie in the case of 2 neighbors of each species.*

#

Build a github repository to store your homework assignments.  Share the link in this file.  

*STUDENT INPUT*

